---
layout: post
title:  Word2vec embedding
date:   2020-05-23 00:00:00 +0800
categories:
tags:
---

Word2vec is the technique to produce word embeddings. It maps a large amount of
words into a vector space, with each word represented by a vector.

# Comparison

Word2vec shines in at least 3 aspects as compared to one-hot encoding:

- word2vec is denser and has a relatively lower dimension as compared to the 
sparse and highly-dimensional one-hot encoding. The dimension of one-hot encoding 
grows with the size of the words (categories). Word2vec is more computationally 
efficient in this sense.

- word2vec maintains the semantics such as cosine similarity of different words,
whereas one-hot encoding does not maintain such information. This is one of the 
biggest advantage in using word2vec in my opinion.

- one-hot encoding is coupled to one application. This makes transfer learning 
difficult as the same words might be mapped to totally different vectors in 
another application.

# Methods

Word2vec is trained with a feed-forward neural network. There are 2 methods:

- **(cbow) continuous bag of words**: Given a list of neighbouring words, 
predict the word surrounded by them. It works better for more frequent works and 
it is generally faster.

- **skip-gram**: Given a word, predict neighbouring words. It works well for 
smaller dataset and for rare words.

# Examples

Training of the word2vec embedding can be done through the 
[gensim library](https://radimrehurek.com/gensim/)

<script src="https://gist.github.com/liusy182/6da472630a7beed11830b887267cc50d.js"></script>

Alternatively, gensim provides a way to load a pre-trained embedding, by using 
the method [load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format). 
In the example below, we load a pre-trained embedding that was trained from 
[google news dataset](https://code.google.com/archive/p/word2vec/).

<script src="https://gist.github.com/liusy182/b8cbe970f35ed7b627102381b488813b.js"></script>

With some additional data cleansing, the loaded model can be fed into the Keras 
as an Embedding layer, with parameter `trainable=False`. More information can be 
found [in this link](https://keras.io/examples/pretrained_word_embeddings/)